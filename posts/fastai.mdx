---
title: Notes on Fast AI
date: 2023-02-28
categories:
  - FastAI
  - Machine Learning
  - Notes
description: Here are some of my notes for Fast AI part 1. I'm going through the course and the book and I'm trying to consolidate everything here.
---

<Callout>
Fast AI is a free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems. <br /><br />These are the notes for Part 1 of the course which can be found [here](https://course.fast.ai/)
</Callout>

I ended up re-reading the book after going through sections of the course and so I thought I would try to consolidate everything here.

## Neural Networks

> Relevant Portions: [Video 1](https://www.youtube.com/watch?v=8SF_h3xF3cE), [Chapter 1](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb), 

Neural Networks are fundamentally mathematical functions - they take in some input and spit out some output. 

<KommyImage blogImage={true} src = "https://pbs.twimg.com/media/FtuMEGpacAAOIil?format=png&name=medium" height = {400} width = {400} />

<StaticTweet id="1647063793200160768" />

We use an objective function - often called a loss function to determine the quality of our mode's predictions. This then allows us to fine tune the parameters of our model.

Important vocabulary to know of is 
- The functional form of the model is called its architecture
- The weights are called parameters
- The predictions are calculated from the independent variables
- The results of the models are called predictions
- The measure of performance is called the loss
- Loss depends on the predictions but also the correct label

Ideally, we want to use a pretrained model when working with a new machine learning problem. This is because our model comes out of the box with certain abilities - eg. ability to recognise edges. 

The Fast.AI vision learner for instance will always remove the last layer of the pretrained model and replace it with a new layer that has randomized weights. This is because the last layer of the model is the one that is most specific to the task at hand.

When working with a model, we want to make sure that we have a validation set. This is a set of data that we don't train our model on. This is important because we want to make sure that our model is able to generalise to data that it hasn't seen before. This helps to prevent overfitting.

### Mechanism

> Relevant Portions : [Chapter 4](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb) 

Neural networks work by starting with a series of randomized weights and then using the gradient of the predictions to adjust the weights. This is called gradient descent.

<KommyImage blogImage = {true} src = "https://github.com/ivanleomk/personal_website_v3/assets/45760326/9d11bae7-b4e1-46c5-b724-46cee702a369" height = "400" width = "400" alt = "Learning Rate Image" />

We then adjust the weights by a set scaled amount of the actual gradient. This is known as the learning rate. 

<Callout>
Choosing the right learning rate is very important! If we choose one that's too small, we'll never be able to reach a good equilibrium. If we choose one that's too big, we'll just end up bouncing around.
</Callout>

We measure the quality of our prediction using a loss function. It's important here to choose the right loss function. **This needs to be a reasonably smooth function without big flat sections and large jumps**. This is because we want to be able to use the gradient of the loss function to adjust our weights.

Often times, we'll be working with large datasets. So we perform this optimisation in minibatches - whereby we operate on portions of our data set and 

## Deployment

> Relevant Portions : 

Deep Learning is a useful method for dealing with problems without a clear solution. Often times, computers are able to find patterns much better than humans can.

Some examples of areas where deep learning has been applied are
- Computer vision
- Natural Language Processing
- Recomender Systems

Always build an end to end pipline before starting on a deep dive and optimisation of a specific component. 


When training a new model, what's most important is to look at the data. There's no need to look towards fancy model architectures before you extract the maximum amount of alpha from your data.

<StaticTweet id="1647275727182770176" />

For instance, Jeremy recommends using the `resnet-18` model as a baseline since it's quick, fast and easy to train.

We can also perform data augmentation - which is a fancy way of saying that we modify our data to either increase the number of samples or to make our model more robust to changes in the data.

Potential ways to do so are
- Random Resizing
- Random Crops
- Rotations

It's important here to make sure that our model will always take in input of a specific size (resolution if you're using images) so make sure u add a rescale step.

### Ethics
> Relevant Portions : [Chapter 3](https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb)

Because deep learning is such a powerful tool and can be used for so many things, it becomes particularly important that we consider the consequences of our choices. The philosophical study of ethics is the study of right and wrong, including how we can define those terms, recognize right and wrong actions, and understand the connection between actions and consequences.

Some examples of where deep learning has had unintended consequences are 
1. Feedback Loops - when youtube's recomendation engine helped to create curated playlists for pedophiles.
2. Bias - when google's ad engine displayed ads for criminal background checks when a traditionally african american name was searched for.


<Callout>
Any dataset involving humans can have this kind of bias: medical data, sales data, housing data, political data, and so on. Because underlying bias is so pervasive, bias in datasets is very pervasive
</Callout>


