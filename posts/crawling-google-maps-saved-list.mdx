---
title: Crawling Your Saved Places with Google Maps
date: 2023-07-01
categories:
  - Selenium
  - Pydantic
  - Web Scraping
description:
  Google Maps sucks at letting you export your saved places. Here's how to do it
---

# Introduction

<Callout>
    As usual, you can find the code for this specific article [here](https://github.com/ivanleomk/scrape-google-maps)
</Callout>

If you've ever used Google Maps, you've definitely struggled to decide where to go to eat. The UI ... frankly sucks beyond belief for an application that has all the data and compute that it has.

<KommyImage src = "/images/Google_Maps.png" height = {400} width = {400} blogImage={true} />

There's not even a simple way to filter the saved places by keywords, categories or even just the location. You need to manually pan and find something that you like.

So i thought I'd crawl it for the fun of it since I wanted more control over my data. I'd seen some jupyter notebooks but no fine-grained script online so I built my own.

## The Idea

When you navigate to a saved location, you see the following

<KommyImage src = "/images/Saved_Location.png" height = {400} width = {400} blogImage={true} />

You get 

1. The name 
2. The overall rating
3. A category that google has assigned to it 

among a few other bits of data. But if you use some of the default categories, the amount of data provided is inconsistent. Therefore, here's how our script gets around inconsistent data.

We simply visit the page that's linked to the listing!

<KommyImage src = "/images/Linked_page.png" height = {400} width = {400} blogImage={true} />

What makes it even better is that the page link itself contains a lot of valuable information 

For instance, take the following link to FiftyFive Coffe Bar,

```bash
https://www.google.com/maps/place/FiftyFive+Coffee+Bar/@1.3731958,103.8075006,12z/data=!4m11!1m3!11m2!2s9wJ7F7C-bjteOlxRQli8-lZz7jeYIw!3e2!3m6!1s0x31da192386bddc8d:0xed0fef7eae8bf927!8m2!3d1.2795647!4d103.8425105!15sCgEqkgEEY2FmZeABAA!16s%2Fg%2F11k50kg36w?entry=ttu
```

We can actually extract out things like the lattitude and longtitude from the link. This helps us to perform useful tasks such as geocoding and reverse geocoding.

Even the HTML isn't too bad for scrapping with unique class names that we can target as seen below 

<KommyImage src = "/images/Google_Html.png" height = {400} width = {400} blogImage={true} />

## The Code

### Data Models

Before we start coding out any code, let's start by defining some models 

<CodeTitle lang = "py" title = "model.py" />
```py
class Status(Enum):
    CLOSED = "Closed"
    TEMPORARILY_CLOSED = "Temporarily Closed"
    OPEN = "Open"


class Listing(BaseModel):
    title: str
    rating: Optional[float]
    number_of_reviews: Optional[int]
    address: Optional[str]
    review: str
    hasDelivery: bool
    hasTakeaway: bool
    hasDineIn: bool
    category: Optional[str]
    status: str
    url: str
    address: str
    long: float
    lat: float
```

The `Status` class is an enumeration that represents the status of a listing. It has three possible values: `CLOSED`, `TEMPORARILY_CLOSED`, and `OPEN`.

The `Listing` class is a Pydantic model that defines the structure and validation rules for the data related to a specific listing. The fields in this model include:

- `title`: The title of the listing (a string)
- `rating`: The rating of the listing (a float, optional)
- `number_of_reviews`: The number of reviews for the listing (an integer, optional)
- `address`: The address of the listing (a string, optional)
- `review`: A review for the listing (a string)
- `hasDelivery`: A boolean indicating if the listing offers delivery
- `hasTakeaway`: A boolean indicating if the listing offers takeaway
- `hasDineIn`: A boolean indicating if the listing offers dine-in
- `category`: The category of the listing (a string, optional)
- `status`: The status of the listing, which should be one of the `Status` enumeration values (a string)
- `url`: The URL of the listing (a string)
- `long`: The longitude of the listing (a float)
- `lat`: The latitude of the listing (a float)

Now that we have our models, let's start by writing some simple code

### Setting up Selenium

> I strongly suggest using a virtual environment to follow along

We can set up a selenium instance to crawl in python by using 

```py
from selenium.webdriver.chrome.service import Service

service = Service()
service.start()

driver = webdriver.Chrome()
list_url = // Your list url
driver.get(list_url)
time.sleep(5)

```

### Crawling the Data

This simply code chunk is enough to navigate to your list url. All our crawler does thereafter is just 

1. Click on each item sequentially 
2. Navigate to the page that links to the item 
3. Extract the data from the page
4. Go back to the original list url 

<KommyImage src = "/images/crawling.gif" height = {400} width = {400} blogImage={true} />


So, before we can even start crawling, it's important to understand how many items we need to crawl. We can do so with a simple while loop 

```py
div_element = driver.find_element(
    By.CSS_SELECTOR,
    'div[class*="m6QErb"][class*="DxyBCb"][class*="kA9KIf"][class*="dS8AEf"]',
)

console.log(f"Starting to crawl list page : {list_url}")
# Scroll down to the specific div element

last_height = driver.execute_script("return arguments[0].scrollHeight", div_element)

while True:
    driver.execute_script(
        "arguments[0].scrollTo(0, arguments[0].scrollHeight)", div_element
    )
    time.sleep(2)

    html_source = div_element.get_attribute("innerHTML")
    curr_height = driver.execute_script("return arguments[0].scrollHeight", div_element)
    if curr_height == last_height:
        break
    last_height = curr_height
```

This simply checks the height of the div which contains all our saved items and sees if it has increased in size each time.


Once we navigate to a specific window, we can then extract all the data that we need.

We first try to find the parent element that contains the data on the listing page 

```python
def extract_details_from_window(driver, review: str) -> Listing:
    try:
        for _ in range(3):
            try:
                driver.find_element(
                    By.CSS_SELECTOR, 'div[class*="m6QErb WNBkOb"]:not(:empty)'
                )
            except Exception as e:
                console.log(
                    "Unable to find parent element. Retrying again in 4 seconds..."
                )
                time.sleep(4)
```

Once we've validated that we've found the parent element, we parse the content using Beautiful Soup 4.

```python
parent_container = driver.find_element(
    By.CSS_SELECTOR, 'div[class*="m6QErb WNBkOb"]:not(:empty)'
)

soup = BeautifulSoup(parent_container.get_attribute("innerHTML"), "html.parser")
is_empty = len(soup.contents) == 0

if is_empty:
    console.log("Parent container is empty")
    raise ValueError("Parent container is empty")
```

In the event that we cannot find any content - this sometimes happens if we just cannot click the item succesfully. We simply raise an error and move on to the next item.

Once we've extracted the data, we can then extract the data from the page. 

Here's an example of the logs for the data data that we extracted from the bearded bella page

```bash
[17:37:18] Unable to find parent element. Retrying again in 4  main.py:70
           seconds...                                                    
           Extracted review as Itâ€™s very good - the cold      main.py:100
           pasta is to die for and the coffee is solid. Solid            
           4/5 would go back                                             
[17:37:24] Extracted out html from parent container           crawl.py:37
           Extracted title as Bearded Bella                   crawl.py:39
           Extracted status as Open                           crawl.py:53
           Extracted rating as 4.2                            crawl.py:66
           Extracted rating as 790                            crawl.py:67
           Extracted address as 8 Craig Rd, Singapore 089668  crawl.py:75
           Extracted lat as 1.2777283 and long as 103.8428438 crawl.py:82
           Extracted category as Restaurant                   crawl.py:96
           Extracted hasDelivery as False                     crawl.py:97
           Extracted hasTakeaway as True                      crawl.py:98
           Extracted hasDineIn as True       
```

### Saving the Data

Once we've extracted out the individual items, we can then write it to a csv file with 

```python
data = [i for i in data if i is not None]
csv_file = "items.csv"
with open(csv_file, "w", newline="", encoding="utf-8-sig") as file:
    writer = csv.writer(file)

    # Write the header row
    writer.writerow(Listing.__fields__.keys())

    # Write the data rows
    for item in data:
        try:
            writer.writerow(item.dict().values())
        except Exception as e:
            print(f"An error occurred while extracting data: {str(e)}")
            import pdb

            pdb.set_trace()

```

> Note that running this specific file will take you quite some time. I suggest running it in the background while you do other things since we implement a good amount of timeouts so we don't get rate limited.


