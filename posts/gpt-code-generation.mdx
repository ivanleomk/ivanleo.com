---
title: Replacing a 5 week-in junior intern
date: 2023-10-23
categories:
  - Machine Learning
  - Retrieval Augmented Generation
description: How to use context-aware RAG and pydantic validation to allow GPT to generate code on a new UI Library
---

## Introduction

A while ago, I saw a demo video of Vercel's V0 and was blown away by what it could produce. It could take in user prompts, feedback and iteratively generate new and improved UI code using the popular `@shadcn/ui` library.

This was soon followed by the [open-v0](https://github.com/raidendotai/openv0/) project by [raidendotai](https://twitter.com/n_raidenai). Since I didn't have access to v0 via vercel, i figured I would clone the project and try to figure out how it worked.

One eventful friday evening later, I ended up putting together a small prototype which uses context-aware RAG and pydantic to generate valid NextJS Code based on a user prompt which you can see below.

> The Gif renders pretty slowly for some reason so if you want to see the original clip, you can check it out [here](https://twitter.com/ivanleomk/status/1715423194973351955)

<KommyImage src="/images/succesful_render.gif" />

### Overview

Let's break down how it works under the hood. On a high level, whenerver a user submits a prompt we do the following

1. First, we extract out a list of substasks which might be relevant to solving this problem. We also pass in a list of components and a short description for each of them into the prompt for added context.

2. We then take these subtasks and embed them, performing a lookup on code chunks in our vector db. This vector db contains a embedding map of `task` ( which was generated by gpt 4 ) to a `code chunk` from the `@shadcn/ui` library.

3. We then perform a deduplication on our code chunk extracted so that the model only sees each example once.

4. We also pass in a set of Rules. These are a set of conditions which the model has to conform to. In my case I found the following to be useful

> Here are some rules that you must follow when generating react code
>
> 1. Always add a title and description to a toast
>
> ```js
>    onClick={() => {
>    toast({
>       title: // title goes here,
>     description: // description,
>     })
> }}
> ```
>
> 2. Make sure to only use imports that follow the following pattern
>
> - 'React'
> - '@/components/ui/componentName`
> - 'next/'
>
> 3.  No other libraries are allowed to be used

5. We then get the model to generate the code therefafter.

Note that the entire codebase used `gpt-4` as a default. I didn't have the time to do fine-tuning.

## Data Preparation

As anyone working with LLMs will tell you, it's most important to have good quality data. In this case, that's very true.

In my case, I chose to process the data as such

1. First, extract out all of the code chunks in the `@shadcn/ui` library. I used a `dump.json` file which was in the `open-v0` repository to do this.

2. Next, for each code chunk, I got GPT-4 to generate a task that might have led to this code chunk being written.

Let's see a quick example.

```js
import { Input } from "@/components/ui/input";

export function InputDemo() {
  return <Input type="email" placeholder="Email" />;
}
```

What are some tasks that could have been assigned to a developer who submits a code chunk like this? In this case, GPT-4 came up with

- Create a new component named `InputDemo` that uses the `Input` component from the @shadcn/ui library
- Set the type of the `Input` component to `email`
- Set the placeholder of the `Input` component to `Email`
- Ensure the `InputDemo` component is exported as the default export

So we perform this task generation for each and every code example that's given in the `@shadcn/ui` library. We ideally want more potential tasks so that for each code chunk, we have more potential options that we can check against when a user makes a query.

### Generating the Tasks

The original `dump.json` file doesn't store code chunks nicely and has a lot more metadata. So we need to first massage the data.

This is done by using the `extract_examples_from_data` function in the source code

```py
def extract_examples_from_data(doc_data):
    return [i["code"] for i in doc_data["docs"]["examples"]]
```

Once we've extracted the source code out, we now have a collection of code chunks. Now let's think a bit about the kind of data structure we expect back. This is where pydantic shines.

```py
class Task(BaseModel):
  """
  This is a class which represents a potential task that could have resulted in the code snippet provided

  eg. I want a button that generates a toast when it's clicked
  eg. I want a login form which allows users to key in their email and validates that it belongs to the facebook.com domain.
  """
  task: str = Field(description="This is a task which might have resulted in the component")
```

it's useful here to point out that the more examples you provide and the more descriptive your class definition is, the better your eventual outputs are going to be. But since we want multiple tasks instead of just one from the code chunk, we can take advantage of the `MultiTask` functionality provided in the `instructor` library.

> Don't forget to run `instructor.patch()` before you run your functions so you get the nice functionality it provides

This allows us to query GPT-4 for multiple instances of the `Task` object by creating a new pydantic class. In our case, we'll call it `MultiTaskType` so that we don't get a naming conflict.

```py
MultiTaskType = instructor.MultiTask(Task)
```

We can then use this in our code as follows

```py
completion = openai.ChatCompletion.create(
  model="gpt-4",
  temperature=0.3,
  stream=False,
  max_retries=2,
  functions=[MultiTaskType.openai_schema],
  function_call={"name": MultiTaskType.openai_schema["name"]},
  messages=[
  {
      "role": "system",
      "content": f"As an experienced programmer using the NextJS Framework with the @shadcn/ui and tailwindcss library, you are tasked with brainstorming some tasks that could have resulted in the following code chunk being produced."
  },
  {
      "role":"assistant",
      "content":"Examples of such tasks could be adding a toast component to display temporary messages, using a specific variant avaliable in the @shadcn/ui library or configuring a component that toggles between two display states"
  },
  {
  "role":"assistant",
  "content": "Tasks should be as diverse as possible while staying relevant. Generate at most 4 Tasks."
      },
  {
      "role": "user",
      "content": f"{chunk}",
  },
  ],
  max_tokens=1000,
  )
  res = MultiTaskType.from_response(completion)
  return [i.task for i in list(res)[0][1]]
```

Notice here that we get automatic retries with the new `max_retries` parameter in the `openai` library by using
