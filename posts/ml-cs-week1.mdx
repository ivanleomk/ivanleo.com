---
title: CS Machine Learning Study Group - Week 1
date: 2023-10-26
categories:
  - MLCS
  - Embeddings
description: Week 1 Study Group Notes
---

## Introduction

I'm starting a new study group at CS. Here's a quick list of the things that we're planning to cover. This article will be updated once we finish the first week

The goal for this week is to learn more about embeddings - particularly how we train them, what they are and how they're able to help us do more.

### Useful Resources

- [Exploring Word2Vec](https://simonwillison.net/2023/Oct/23/embeddings/#exploring-how-these-things-work-with-word2vec)
- [Training your own Word2Vec](https://jaketae.github.io/study/word2vec/)
- [What are Embeddings](https://simonwillison.net/2023/Oct/23/embeddings/#what-are-embeddings)
- [What are Rotational Positional Embeddings](https://paperswithcode.com/method/rope)
- [Transformer Positional Embeddings](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
- [Customizing Black Box Embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb)
- [The Annotated Clip Part 1](https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html) - There's a part 2 somewhere there
- [Embeddings vs One-Hot Encoding](https://betterprogramming.pub/feature-engineering-using-onehot-embedding-in-julia-dadbc1bfe861#:~:text=In%20short%2C%20embedding%20is%20more,feature%20vectors%20without%20losing%20information)
- [Embeddings for Next Token Prediction](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

I'd love to also talk a bit about how we might be able to reproduce this [model](https://github.com/MF-FOOM/wikivec2text) that uses nanogpt or this other [one](https://huggingface.co/collections/thesephist/contra-bottleneck-t5-651b5f1e26b2dee1ed16429e) which uses a finetuned t5 model to embed and generate text.
